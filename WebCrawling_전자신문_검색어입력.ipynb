{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c97eb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어를 입력하세요 : 전기차\n"
     ]
    }
   ],
   "source": [
    "def han_article():\n",
    "    # 모듈 임포트\n",
    "    import urllib.request\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    import os\n",
    "\n",
    "    # 검색어 입력\n",
    "    search_text = input(\"검색어를 입력하세요 : \").encode(\"utf-8\")\n",
    "    search_text = str(search_text)[2:-1].replace('\\\\x', '%')\n",
    "\n",
    "    ##상세 기사 url   \n",
    "    url_list = []\n",
    "    for i in range(30):\n",
    "        list_url = \"https://search.etnews.com/etnews/search.html?category=CATEGORY1&kwd=\"+search_text+\"&pageNum=\"+str(i)+\"&pageSize=10&reSrchFlag=false&sort=1&startDate=&endDate=&sitegubun=&jisikgubun=&preKwd%5B0%5D=\"+search_text\n",
    "\n",
    "        url = urllib.request.Request(list_url)  # url 요청\n",
    "        res = urllib.request.urlopen(url).read().decode(\"utf-8\")  # utf 파일로 decoding\n",
    "\n",
    "        soup = BeautifulSoup(res, \"html.parser\")  # 전체 html\n",
    "\n",
    "        ##url_list에 url 전체 담기\n",
    "        for link in soup.find_all('dt'):  # dt 태그에 해당하는 모든 부분\n",
    "            for i in link:  # resultset를 태그로 만드는 작업 \n",
    "                            #dt 태그 밑에 있는 a 태그를 가져오기 위해 for loop\n",
    "                url_list.append(i.get('href'))  # 태그가 되어야 get, get_text()를 쓸 수 있다.\n",
    "\n",
    "    ##상세 기사    \n",
    "    full_article = []\n",
    "\n",
    "    # url 하나씩 불러오기\n",
    "    for i in range(len(url_list)):\n",
    "        url = urllib.request.Request(url_list[i])\n",
    "        res = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "\n",
    "        # print(res)  # 위의 두가지 작업을 거치면 \n",
    "        # 위의 url 의 html 문서를 res 변수에 담을수 있게 된다.\n",
    "        \n",
    "        soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "        day = []\n",
    "        article = []\n",
    "\n",
    "        for link1, link2 in zip(soup.find_all('time', class_=\"date\"), soup.find_all('div', class_=\"article_txt\")):\n",
    "            day.append(link1.get_text())\n",
    "            article.append(link2.get_text())\n",
    "\n",
    "        for i, j in zip(day, article):\n",
    "            full_article.append(i.strip())\n",
    "            full_article.append(j.strip())\n",
    "\n",
    "    f = open('etnews_전기차.txt', 'w', encoding='UTF-8') # 키워드에 맞춰 파일명 변경하기\n",
    "    f.writelines(full_article)\n",
    "    f.close()\n",
    "\n",
    "han_article()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7002c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
